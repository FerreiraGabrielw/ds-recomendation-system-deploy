---
title: "Hybrid Recommender System for E-commerce (End-to-End)"
author: "Gabriel Ferreira"
date: "2025-12-19"
format:
  html:
    page-layout: full
    code-fold: true
    code-summary: "Show Code"
    toc: true
    toc-depth: 1
    toc-title: "Project Index"
    anchor-sections: false
engine: jupytern
jupyter: python3
---

# Introduction

Este projeto apresenta o desenvolvimento de um sistema de recomenda√ß√£o h√≠brido para e-commerce, cobrindo todo o ciclo de vida de uma solu√ß√£o de Machine Learning aplicada em contexto real de neg√≥cio.

O objetivo central √© aumentar convers√£o e ticket m√©dio por meio de recomenda√ß√µes personalizadas, utilizando m√∫ltiplos sinais complementares:

* comportamento hist√≥rico de compra (Collaborative Filtering),
* similaridade sem√¢ntica entre produtos (Content-Based),
* padr√µes expl√≠citos de coocorr√™ncia em cestas de compra (Association Rules).

O projeto foi desenhado com foco em produ√ß√£o, contemplando:

* modelagem relacional,
* explora√ß√£o de dados em SQL,
* avalia√ß√£o offline,
* treinamento offline e deploy em AWS.

---

# Project Structure

O pipeline segue uma arquitetura end-to-end t√≠pica de sistemas de recomenda√ß√£o modernos:

1. Defini√ß√£o do problema de neg√≥cio
2. Gera√ß√£o e ingest√£o dos dados
3. Explora√ß√£o e valida√ß√£o em SQL
4. Feature Engineering
5. Modelagem (CF, CB, AR)
6. Avalia√ß√£o offline
7. Constru√ß√£o do modelo h√≠brido
8. Prepara√ß√£o de artefatos
9. Deploy em cloud (AWS)

Essa estrutura garante separa√ß√£o clara entre dados, modelos e infer√™ncia, facilitando manuten√ß√£o e escalabilidade.

---

# Business Objective & KPIs

**Objetivo de neg√≥cio:**
Aumentar a relev√¢ncia das recomenda√ß√µes para impulsionar:

* AOV (Average Order Value),
* taxa de convers√£o,
* CTR de recomenda√ß√µes,
* receita incremental.

---

# Data Sources & Modeling Assumptions

Os dados utilizados representam um ambiente transacional t√≠pico de e-commerce, com tabelas normalizadas que refletem pr√°ticas comuns de armazenamento e an√°lise anal√≠tica.

O dataset inclui:

* hist√≥rico de transa√ß√µes em n√≠vel de item,
* cat√°logo de produtos com atributos descritivos,
* eventos de visualiza√ß√£o de produtos,
* informa√ß√µes demogr√°ficas b√°sicas de clientes,
* registros temporais completos para an√°lise comportamental.

---

# Database Setup & SQL Exploration

Antes de qualquer modelagem em Python, os dados s√£o analisados diretamente no **MySQL**, garantindo integridade e compreens√£o do comportamento do neg√≥cio.

---

```{python}
#| include: false
from getpass import getpass
import os
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict
import random
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
import pickle
```


## SQL Connection & Helper Functions
```{python}
#| include: false
# Conectar no SQL
DB_USER = "root"
DB_HOST = "127.0.0.1"
DB_PORT = 3306
DB_NAME = "ecommerce_db"
DB_PASS = "sqlmala123"

CONN_STR = f"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
engine = create_engine(CONN_STR, echo=False)

# Pra reprodutibilidade
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
```

##### Fun√ß√£o Python para rodar SQL puro no notebook:
```{python}
def run_sql_pure(sql: str, params: dict = None):
    """
    Executa SQL puro (string) e retorna um pandas.DataFrame.
    Exibe a query (√∫til para portf√≥lio) e retorna o DataFrame.
    """
    print("---- Executing SQL ----")
    print(sql.strip())
    print("-----------------------")
    df = pd.read_sql_query(sql=text(sql), con=engine, params=params)
    display(df.head(10))
    return df

# Configura√ß√£o do matplotlib
plt.rcParams['figure.figsize'] = (10,5)
plt.rcParams['grid.linestyle'] = '--'
```

---

## EDA (Exploratory Data Analysis)

O primeiro passo da EDA √© validar a integridade estrutural das tabelas.

##### Contagens por tabela:
```{python}
sql = """
SELECT 
  (SELECT COUNT(*) FROM customers) AS n_customers,
  (SELECT COUNT(*) FROM products)  AS n_products,
  (SELECT COUNT(*) FROM transactions) AS n_transactions,
  (SELECT COUNT(*) FROM product_views) AS n_views;
"""
df = run_sql_pure(sql)
vals = df.iloc[0].to_dict()
names = list(vals.keys()); counts = list(vals.values())
```
<br>

##### Valores ausentes por tabela
```{python}
# lista de tabelas a verificar
tables = ['customers', 'products', 'transactions', 'product_views']

results = []
for t in tables:
    q_cols = f"""
    SELECT COLUMN_NAME
    FROM INFORMATION_SCHEMA.COLUMNS
    WHERE TABLE_SCHEMA = '{DB_NAME}'
      AND TABLE_NAME = '{t}'
    ORDER BY ORDINAL_POSITION;
    """
    cols_df = pd.read_sql_query(sql=text(q_cols), con=engine)
    cols = cols_df['COLUMN_NAME'].tolist()
    num_cols = len(cols)

    # contar linhas
    q_rows = f"SELECT COUNT(*) AS n_rows FROM {t};"
    n_rows = pd.read_sql_query(sql=text(q_rows), con=engine).iloc[0,0]

    if n_rows == 0 or num_cols == 0:
        null_count = 0
    else:
        null_expr = " + ".join([f"SUM(`{c}` IS NULL)" for c in cols])
        q_nulls = f"SELECT {null_expr} AS null_count FROM {t};"
        null_count = pd.read_sql_query(sql=text(q_nulls), con=engine).iloc[0,0]

    total_cells = int(n_rows) * int(num_cols)
    pct_null = (null_count / total_cells * 100) if total_cells > 0 else 0.0

    results.append({
        "table": t,
        "n_rows": int(n_rows),
        "n_cols": int(num_cols),
        "total_cells": int(total_cells),
        "null_count": int(null_count),
        "pct_null": round(pct_null, 4)
    })

df_missing_tables = pd.DataFrame(results)
display(df_missing_tables)
```
<br>

##### Duplicados e chaves (checar chave prim√°ria √∫nica)
```{python}
sql = """
SELECT 
  COUNT(*) AS total_rows,
  COUNT(DISTINCT transaction_id) AS distinct_transaction_id,
  COUNT(DISTINCT CONCAT(customer_id,'-',DATE(transaction_date))) AS distinct_cust_date
FROM transactions;
"""
df = run_sql_pure(sql)
print(df.to_string(index=False))
```
<br>

Os resultados confirmam que os dados est√£o completos e consistentes, permitindo avan√ßar para as an√°lises sem necessidade de limpeza estrutural.

---

### Visualiza√ß√µes e Insights
Nesta etapa, exploramos o comportamento do neg√≥cio por meio de SQL, respondendo perguntas como:
* Quais produtos vendem mais?
* Onde est√° concentrada a receita?
* Existe sazonalidade?
* Como os clientes se distribuem em termos de rec√™ncia, frequ√™ncia e gasto?
* Qual o n√≠vel de sparsity da matriz cliente √ó produto?

##### Distribui√ß√£o de pre√ßos - produtos
```{python}
sql = "SELECT price FROM products WHERE price IS NOT NULL;"
df_price = run_sql_pure(sql)
df_price['price'] = pd.to_numeric(df_price['price'], errors='coerce')
plt.hist(df_price['price'].dropna(), bins=30)
plt.title("Product Price Distribuction")
plt.xlabel("Price")
plt.ylabel("Frequency")
plt.show()
```
<br>

##### Top Produtos por quantidade e por receita
```{python}
sql = """
SELECT p.product_id, p.name,
       SUM(t.quantity) AS total_qty,
       SUM(t.total_value) AS total_revenue
FROM transactions t
JOIN products p ON t.product_id = p.product_id
GROUP BY p.product_id
ORDER BY total_qty DESC
LIMIT 10;
"""
df_top = run_sql_pure(sql)
ax = df_top.plot.bar(x='name', y='total_qty', legend=False)
plt.xticks(rotation=70, fontsize=8)
plt.title("Top 15 produtos por quantidade vendida")
plt.ylabel("Quantidade")
plt.show()

# revenue bar
df_top.plot.bar(x='name', y='total_revenue')
plt.xticks(rotation=70, fontsize=8)
plt.title("Top 15 produtos por receita (mesmos produtos)")
plt.ylabel("Receita")
plt.show()
```
<br>

##### Vendas por m√™s
```{python}
sql = """
SELECT DATE_FORMAT(transaction_date, '%Y-%m') AS ym,
       COUNT(*) AS n_items,
       SUM(total_value) AS revenue
FROM transactions
GROUP BY ym
ORDER BY ym;
"""
df_ts = run_sql_pure(sql)
df_ts['ym'] = pd.to_datetime(df_ts['ym'] + '-01')
plt.plot(df_ts['ym'], df_ts['revenue'])
plt.title("Receita por m√™s")
plt.xlabel("M√™s")
plt.ylabel("Receita")
plt.grid(True)
plt.show()
```
<br>

##### RFM - tabela e histogramas 
```{python}
sql = """
WITH last AS (
  SELECT customer_id,
         DATEDIFF(CURDATE(), MAX(DATE(transaction_date))) AS recency_days,
         COUNT(*) AS frequency,
         SUM(total_value) AS monetary
  FROM transactions
  GROUP BY customer_id
)
SELECT * FROM last;
"""
df_rfm = run_sql_pure(sql)
# Histograms
fig, axes = plt.subplots(1,3, figsize=(15,4))
axes[0].hist(df_rfm['recency_days'].dropna(), bins=20); axes[0].set_title('Recency (dias)')
axes[1].hist(df_rfm['frequency'].dropna(), bins=20); axes[1].set_title('Frequency')
axes[2].hist(df_rfm['monetary'].dropna(), bins=20); axes[2].set_title('Monetary')
plt.show()

# scatter recency x monetary
plt.scatter(df_rfm['recency_days'], df_rfm['monetary'])
plt.xlabel('Recency (dias)'); plt.ylabel('Monetary (total_spent)')
plt.title('Recency x Monetary (clientes)')
plt.grid(True)
plt.show()
```
<br>

##### Distribui√ß√£o de compras por cliente 
```{python}
sql = """
SELECT customer_id, COUNT(*) AS purchases
FROM transactions
GROUP BY customer_id;
"""
df_purchases = run_sql_pure(sql)
plt.hist(df_purchases['purchases'], bins=30)
plt.title("Distribui√ß√£o de n√∫mero de compras por cliente")
plt.xlabel("N√∫mero de compras")
plt.ylabel("Clientes")
plt.show()
```
<br>

##### Convers√£o views -> buys por produto
```{python}
sql = """
SELECT p.product_id, p.name,
  COALESCE(v.views,0) AS views,
  COALESCE(b.buys,0)  AS buys,
  ROUND(COALESCE(b.buys,0) / NULLIF(COALESCE(v.views,0),0) * 100,2) AS conversion_pct
FROM products p
LEFT JOIN (
  SELECT product_id, COUNT(*) AS views
  FROM product_views
  GROUP BY product_id
) v ON p.product_id = v.product_id
LEFT JOIN (
  SELECT product_id, COUNT(*) AS buys
  FROM transactions
  GROUP BY product_id
) b ON p.product_id = b.product_id;
"""
df_conv = run_sql_pure(sql)
plt.scatter(df_conv['views'], df_conv['buys'])
plt.xlabel('Views'); plt.ylabel('Buys')
plt.title('Views x Buys (produtos)')
plt.grid(True)
plt.show()

# print top products with unusually high conversion (>80%)
high_conv = df_conv[df_conv['conversion_pct'] > 80].sort_values('conversion_pct', ascending=False)
print("Produtos com convers√£o > 80% (exemplo):")
display(high_conv[['product_id','name','views','buys','conversion_pct']].head(10))
```
<br>

##### Vendas por categoria 
```{python}
sql = """
SELECT p.category, COUNT(*) AS n_items, SUM(t.total_value) AS revenue
FROM transactions t
JOIN products p ON t.product_id = p.product_id
GROUP BY p.category
ORDER BY revenue DESC;
"""
df_cat = run_sql_pure(sql)
df_cat.plot.bar(x='category', y='revenue')
plt.title("Receita por categoria")
plt.xticks(rotation=60)
plt.ylabel("Receita")
plt.show()
```
<br>

##### AOV (Average Order Value)
```{python}
sql = """
SELECT customer_id, DATE(transaction_date) AS order_date, SUM(total_value) AS order_value
FROM transactions
GROUP BY customer_id, DATE(transaction_date);
"""
df_orders = run_sql_pure(sql)
aov = df_orders['order_value'].mean()
print(f"AOV (m√©dia de valor por pedido - aproxima√ß√£o): R$ {aov:.2f}")
plt.hist(df_orders['order_value'], bins=30)
plt.title("Distribui√ß√£o do valor por pedido (aprox.)")
plt.xlabel("Order value")
plt.ylabel("Contagem")
plt.show()
```
<br>

##### Matriz cliente x produto - densidade (SPARSITY)
```{python}
sql = """
SELECT (SELECT COUNT(DISTINCT customer_id) FROM transactions) AS n_customers_active,
       (SELECT COUNT(DISTINCT product_id) FROM transactions) AS n_products_sold,
       (SELECT COUNT(*) FROM transactions) AS n_transactions;
"""
df_dim = run_sql_pure(sql)
n_customers = int(df_dim['n_customers_active'].iloc[0])
n_products = int(df_dim['n_products_sold'].iloc[0])
n_trans = int(df_dim['n_transactions'].iloc[0])
possible = n_customers * n_products
sparsity = 1 - (n_trans / possible)
print(f"Clientes: {n_customers}, Produtos: {n_products}, Transa√ß√µes: {n_trans}")
print(f"Sparsity (aprox): {sparsity:.6f}")
```

---

## Key Insights from SQL EDA

A an√°lise revela padr√µes importantes:

* alta concentra√ß√£o de vendas em poucas categorias,
* clientes com frequ√™ncia relativamente elevada,
* sparsity alta (>90%) na matriz de intera√ß√µes.

Esses achados justificam a escolha por:

* modelos **item-based** em vez de user-based,
* abordagem h√≠brida para mitigar limita√ß√µes individuais.

---

# Cria√ß√£o dos Algoritimos

Com base na EDA, foi definida uma arquitetura h√≠brida onde cada componente resolve um problema espec√≠fico:

| Componente        | Papel                                   |
| ----------------- | --------------------------------------- |
| Item-Item CF      | Personaliza√ß√£o baseada em comportamento |
| Content-Based     | Similaridade sem√¢ntica e cold start     |
| Association Rules | Cross-sell contextual                   |
<br>

O objetivo n√£o √© substituir modelos, mas **combinar sinais complementares**.


## Item-Item Collaborative Filtering

Nesta etapa, constru√≠mos um sistema de recomenda√ß√£o baseado exclusivamente em padr√µes hist√≥ricos de compra. O Item-Item CF recomenda itens semelhantes aos que o usu√°rio j√° comprou, medindo similaridade entre itens a partir do comportamento dos usu√°rios (quem comprou A tamb√©m comprou B).

```{python}
#| include: false
# Carregando Datasets
customers = pd.read_csv("customers.csv")
products = pd.read_csv("products.csv")
transactions = pd.read_csv("transactions.csv")
views = pd.read_csv("product_views.csv")
```





---
```{python}

```
<br>



---


---



‚¨áÔ∏è **INSIRA O C√ìDIGO AQUI (matriz item√óuser, cosine, shrinkage)**

---

## Interpretation ‚Äî CF

O score gerado pelo modelo representa **afinidade relativa entre produtos**, n√£o probabilidade absoluta.

O uso de shrinkage reduz ru√≠do causado por coocorr√™ncias raras, tornando o ranking mais confi√°vel.

---

# Content-Based Filtering (TF-IDF)

Apesar da for√ßa do sinal comportamental, o CF apresenta limita√ß√µes:

* cold start de produtos,
* baixa explicabilidade sem√¢ntica.

Para mitigar isso, utilizamos Content-Based Filtering, representando produtos por seus atributos textuais.

‚¨áÔ∏è **INSIRA O C√ìDIGO AQUI (TF-IDF + cosine)**

---

## Interpretation ‚Äî CB

O modelo baseado em conte√∫do recomenda produtos semanticamente similares, ampliando a cobertura do sistema e permitindo recomenda√ß√µes mesmo para itens com pouco hist√≥rico.

---

# Offline Evaluation ‚Äî CF vs CB

Antes da constru√ß√£o do modelo h√≠brido, os modelos s√£o avaliados separadamente utilizando:

* holdout temporal (leave-one-out),
* m√©trica Hit Rate@K.

‚¨áÔ∏è **INSIRA O C√ìDIGO AQUI (avalia√ß√£o CF vs CB)**

**Conclus√£o:**
O CF apresenta melhor desempenho isolado, enquanto o CB agrega valor incremental ‚Äî justificando a combina√ß√£o.

---

# Hybrid Model ‚Äî Weight Tuning

Nesta etapa, testamos diferentes combina√ß√µes de pesos entre CF e CB para maximizar performance.

‚¨áÔ∏è **INSIRA O C√ìDIGO AQUI (tuning de pesos)**

O melhor resultado √© obtido com predomin√¢ncia do sinal colaborativo, complementado pelo conte√∫do.

---

# Association Rules (Apriori)

As regras de associa√ß√£o capturam padr√µes expl√≠citos de coocorr√™ncia em cestas de compra, sendo especialmente √∫teis para estrat√©gias de cross-sell.

‚¨áÔ∏è **INSIRA O C√ìDIGO AQUI (Apriori + rules)**

**Uso controlado:**
As regras s√£o aplicadas apenas quando h√° correspond√™ncia real, evitando recomenda√ß√µes artificiais.

---

# Final Hybrid Recommender

O modelo final combina:

* CF (personaliza√ß√£o),
* CB (sem√¢ntica),
* AR (contexto de compra).

As pontua√ß√µes s√£o normalizadas e combinadas via soma ponderada.

‚¨áÔ∏è **INSIRA O C√ìDIGO AQUI (modelo h√≠brido final)**

---

# Offline Artifacts & Deployment Strategy

Os modelos s√£o treinados offline e serializados como artefatos est√°ticos.

Em produ√ß√£o:

> *A API apenas carrega artefatos e executa ranking.*

‚¨áÔ∏è **INSIRA O C√ìDIGO AQUI (serializa√ß√£o)**

---

# Cloud Deployment (AWS)

A solu√ß√£o √© implantada em arquitetura serverless:

* S3 para armazenamento de artefatos,
* Lambda para infer√™ncia,
* API Gateway para exposi√ß√£o REST.

üé• **Video ‚Äî AWS Deploy & API Demo**

```markdown
{{< video aws_demo.mp4 >}}
```

---

# Conclusion

Este projeto demonstra a constru√ß√£o de um sistema de recomenda√ß√£o h√≠brido **robusto, interpret√°vel e orientado √† produ√ß√£o**, combinando rigor t√©cnico com impacto direto no neg√≥cio.

A abordagem h√≠brida permite equilibrar personaliza√ß√£o, cobertura e oportunidades de cross-sell, refletindo pr√°ticas reais adotadas em plataformas de e-commerce em escala.

---

## GitHub Repository

[Link para o reposit√≥rio]

---


