---
title: "Hybrid Recommender System for E-commerce (End-to-End)"
author: "Gabriel Ferreira"
date: "2025-12-19"
format:
  html:
    page-layout: full
    code-fold: true
    code-summary: "Show Code"
    toc: true
    toc-depth: 1
    toc-title: "Project Index"
    anchor-sections: false
engine: jupytern
jupyter: python3
---

# Introduction

Este projeto apresenta o desenvolvimento de um sistema de recomenda√ß√£o h√≠brido para e-commerce, cobrindo todo o ciclo de vida de uma solu√ß√£o de Machine Learning aplicada em contexto real de neg√≥cio.

O objetivo central √© aumentar convers√£o e ticket m√©dio por meio de recomenda√ß√µes personalizadas, utilizando m√∫ltiplos sinais complementares:

* comportamento hist√≥rico de compra (Collaborative Filtering),
* similaridade sem√¢ntica entre produtos (Content-Based),
* padr√µes expl√≠citos de coocorr√™ncia em cestas de compra (Association Rules).

O projeto foi desenhado com foco em produ√ß√£o, contemplando:

* modelagem relacional,
* explora√ß√£o de dados em SQL,
* avalia√ß√£o offline,
* treinamento offline e deploy em AWS.

---

# Project Structure

O pipeline segue uma arquitetura end-to-end t√≠pica de sistemas de recomenda√ß√£o modernos:

1. Defini√ß√£o do problema de neg√≥cio
2. Gera√ß√£o e ingest√£o dos dados
3. Explora√ß√£o e valida√ß√£o em SQL
4. Feature Engineering
5. Modelagem (CF, CB, AR)
6. Avalia√ß√£o offline
7. Constru√ß√£o do modelo h√≠brido
8. Prepara√ß√£o de artefatos
9. Deploy em cloud (AWS)

Essa estrutura garante separa√ß√£o clara entre dados, modelos e infer√™ncia, facilitando manuten√ß√£o e escalabilidade.

---

# Business Objective & KPIs

**Objetivo de neg√≥cio:**
Aumentar a relev√¢ncia das recomenda√ß√µes para impulsionar:

* AOV (Average Order Value),
* taxa de convers√£o,
* CTR de recomenda√ß√µes,
* receita incremental.

---

# Data Sources & Modeling Assumptions

Os dados utilizados representam um ambiente transacional t√≠pico de e-commerce, com tabelas normalizadas que refletem pr√°ticas comuns de armazenamento e an√°lise anal√≠tica.

O dataset inclui:

* hist√≥rico de transa√ß√µes em n√≠vel de item,
* cat√°logo de produtos com atributos descritivos,
* eventos de visualiza√ß√£o de produtos,
* informa√ß√µes demogr√°ficas b√°sicas de clientes,
* registros temporais completos para an√°lise comportamental.

---

# Database Setup & SQL Exploration

Antes de qualquer modelagem em Python, os dados s√£o analisados diretamente no **MySQL**, garantindo integridade e compreens√£o do comportamento do neg√≥cio.

---

```{python}
#| include: false
from getpass import getpass
import os
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict
import random
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
import pickle
```


## SQL Connection & Helper Functions
```{python}
#| include: false
# Conectar no SQL
DB_USER = "root"
DB_HOST = "127.0.0.1"
DB_PORT = 3306
DB_NAME = "ecommerce_db"
DB_PASS = "sqlmala123"

CONN_STR = f"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
engine = create_engine(CONN_STR, echo=False)

# Pra reprodutibilidade
SEED = 42
np.random.seed(SEED)
random.seed(SEED)
```

##### Fun√ß√£o Python para rodar SQL puro no notebook:
```{python}
def run_sql_pure(sql: str, params: dict = None):
    """
    Executa SQL puro (string) e retorna um pandas.DataFrame.
    Exibe a query (√∫til para portf√≥lio) e retorna o DataFrame.
    """
    print("---- Executing SQL ----")
    print(sql.strip())
    print("-----------------------")
    df = pd.read_sql_query(sql=text(sql), con=engine, params=params)
    display(df.head(10))
    return df

# Configura√ß√£o do matplotlib
plt.rcParams['figure.figsize'] = (10,5)
plt.rcParams['grid.linestyle'] = '--'
```

---

## EDA (Exploratory Data Analysis)

O primeiro passo da EDA √© validar a integridade estrutural das tabelas.

##### Contagens por tabela:
```{python}
sql = """
SELECT 
  (SELECT COUNT(*) FROM customers) AS n_customers,
  (SELECT COUNT(*) FROM products)  AS n_products,
  (SELECT COUNT(*) FROM transactions) AS n_transactions,
  (SELECT COUNT(*) FROM product_views) AS n_views;
"""
df = run_sql_pure(sql)
vals = df.iloc[0].to_dict()
names = list(vals.keys()); counts = list(vals.values())
```
<br>

##### Valores ausentes por tabela
```{python}
# lista de tabelas a verificar
tables = ['customers', 'products', 'transactions', 'product_views']

results = []
for t in tables:
    q_cols = f"""
    SELECT COLUMN_NAME
    FROM INFORMATION_SCHEMA.COLUMNS
    WHERE TABLE_SCHEMA = '{DB_NAME}'
      AND TABLE_NAME = '{t}'
    ORDER BY ORDINAL_POSITION;
    """
    cols_df = pd.read_sql_query(sql=text(q_cols), con=engine)
    cols = cols_df['COLUMN_NAME'].tolist()
    num_cols = len(cols)

    # contar linhas
    q_rows = f"SELECT COUNT(*) AS n_rows FROM {t};"
    n_rows = pd.read_sql_query(sql=text(q_rows), con=engine).iloc[0,0]

    if n_rows == 0 or num_cols == 0:
        null_count = 0
    else:
        null_expr = " + ".join([f"SUM(`{c}` IS NULL)" for c in cols])
        q_nulls = f"SELECT {null_expr} AS null_count FROM {t};"
        null_count = pd.read_sql_query(sql=text(q_nulls), con=engine).iloc[0,0]

    total_cells = int(n_rows) * int(num_cols)
    pct_null = (null_count / total_cells * 100) if total_cells > 0 else 0.0

    results.append({
        "table": t,
        "n_rows": int(n_rows),
        "n_cols": int(num_cols),
        "total_cells": int(total_cells),
        "null_count": int(null_count),
        "pct_null": round(pct_null, 4)
    })

df_missing_tables = pd.DataFrame(results)
display(df_missing_tables)
```
<br>

##### Duplicados e chaves (checar chave prim√°ria √∫nica)
```{python}
sql = """
SELECT 
  COUNT(*) AS total_rows,
  COUNT(DISTINCT transaction_id) AS distinct_transaction_id,
  COUNT(DISTINCT CONCAT(customer_id,'-',DATE(transaction_date))) AS distinct_cust_date
FROM transactions;
"""
df = run_sql_pure(sql)
print(df.to_string(index=False))
```
<br>

Os resultados confirmam que os dados est√£o completos e consistentes, permitindo avan√ßar para as an√°lises sem necessidade de limpeza estrutural.

---

### Visualiza√ß√µes e Insights
Nesta etapa, exploramos o comportamento do neg√≥cio por meio de SQL, respondendo perguntas como:
* Quais produtos vendem mais?
* Onde est√° concentrada a receita?
* Existe sazonalidade?
* Como os clientes se distribuem em termos de rec√™ncia, frequ√™ncia e gasto?
* Qual o n√≠vel de sparsity da matriz cliente √ó produto?

##### Distribui√ß√£o de pre√ßos - produtos
```{python}
sql = "SELECT price FROM products WHERE price IS NOT NULL;"
df_price = run_sql_pure(sql)
df_price['price'] = pd.to_numeric(df_price['price'], errors='coerce')
plt.hist(df_price['price'].dropna(), bins=30)
plt.title("Product Price Distribuction")
plt.xlabel("Price")
plt.ylabel("Frequency")
plt.show()
```
<br>

##### Top Produtos por quantidade e por receita
```{python}
sql = """
SELECT p.product_id, p.name,
       SUM(t.quantity) AS total_qty,
       SUM(t.total_value) AS total_revenue
FROM transactions t
JOIN products p ON t.product_id = p.product_id
GROUP BY p.product_id
ORDER BY total_qty DESC
LIMIT 10;
"""
df_top = run_sql_pure(sql)
ax = df_top.plot.bar(x='name', y='total_qty', legend=False)
plt.xticks(rotation=70, fontsize=8)
plt.title("Top 15 produtos por quantidade vendida")
plt.ylabel("Quantidade")
plt.show()

# revenue bar
df_top.plot.bar(x='name', y='total_revenue')
plt.xticks(rotation=70, fontsize=8)
plt.title("Top 15 produtos por receita (mesmos produtos)")
plt.ylabel("Receita")
plt.show()
```
<br>

##### Vendas por m√™s
```{python}
sql = """
SELECT DATE_FORMAT(transaction_date, '%Y-%m') AS ym,
       COUNT(*) AS n_items,
       SUM(total_value) AS revenue
FROM transactions
GROUP BY ym
ORDER BY ym;
"""
df_ts = run_sql_pure(sql)
df_ts['ym'] = pd.to_datetime(df_ts['ym'] + '-01')
plt.plot(df_ts['ym'], df_ts['revenue'])
plt.title("Receita por m√™s")
plt.xlabel("M√™s")
plt.ylabel("Receita")
plt.grid(True)
plt.show()
```
<br>

##### RFM - tabela e histogramas 
```{python}
sql = """
WITH last AS (
  SELECT customer_id,
         DATEDIFF(CURDATE(), MAX(DATE(transaction_date))) AS recency_days,
         COUNT(*) AS frequency,
         SUM(total_value) AS monetary
  FROM transactions
  GROUP BY customer_id
)
SELECT * FROM last;
"""
df_rfm = run_sql_pure(sql)
# Histograms
fig, axes = plt.subplots(1,3, figsize=(15,4))
axes[0].hist(df_rfm['recency_days'].dropna(), bins=20); axes[0].set_title('Recency (dias)')
axes[1].hist(df_rfm['frequency'].dropna(), bins=20); axes[1].set_title('Frequency')
axes[2].hist(df_rfm['monetary'].dropna(), bins=20); axes[2].set_title('Monetary')
plt.show()

# scatter recency x monetary
plt.scatter(df_rfm['recency_days'], df_rfm['monetary'])
plt.xlabel('Recency (dias)'); plt.ylabel('Monetary (total_spent)')
plt.title('Recency x Monetary (clientes)')
plt.grid(True)
plt.show()
```
<br>

##### Distribui√ß√£o de compras por cliente 
```{python}
sql = """
SELECT customer_id, COUNT(*) AS purchases
FROM transactions
GROUP BY customer_id;
"""
df_purchases = run_sql_pure(sql)
plt.hist(df_purchases['purchases'], bins=30)
plt.title("Distribui√ß√£o de n√∫mero de compras por cliente")
plt.xlabel("N√∫mero de compras")
plt.ylabel("Clientes")
plt.show()
```
<br>

##### Convers√£o views -> buys por produto
```{python}
sql = """
SELECT p.product_id, p.name,
  COALESCE(v.views,0) AS views,
  COALESCE(b.buys,0)  AS buys,
  ROUND(COALESCE(b.buys,0) / NULLIF(COALESCE(v.views,0),0) * 100,2) AS conversion_pct
FROM products p
LEFT JOIN (
  SELECT product_id, COUNT(*) AS views
  FROM product_views
  GROUP BY product_id
) v ON p.product_id = v.product_id
LEFT JOIN (
  SELECT product_id, COUNT(*) AS buys
  FROM transactions
  GROUP BY product_id
) b ON p.product_id = b.product_id;
"""
df_conv = run_sql_pure(sql)
plt.scatter(df_conv['views'], df_conv['buys'])
plt.xlabel('Views'); plt.ylabel('Buys')
plt.title('Views x Buys (produtos)')
plt.grid(True)
plt.show()

# print top products with unusually high conversion (>80%)
high_conv = df_conv[df_conv['conversion_pct'] > 80].sort_values('conversion_pct', ascending=False)
print("Produtos com convers√£o > 80% (exemplo):")
display(high_conv[['product_id','name','views','buys','conversion_pct']].head(10))
```
<br>

##### Vendas por categoria 
```{python}
sql = """
SELECT p.category, COUNT(*) AS n_items, SUM(t.total_value) AS revenue
FROM transactions t
JOIN products p ON t.product_id = p.product_id
GROUP BY p.category
ORDER BY revenue DESC;
"""
df_cat = run_sql_pure(sql)
df_cat.plot.bar(x='category', y='revenue')
plt.title("Receita por categoria")
plt.xticks(rotation=60)
plt.ylabel("Receita")
plt.show()
```
<br>

##### AOV (Average Order Value)
```{python}
sql = """
SELECT customer_id, DATE(transaction_date) AS order_date, SUM(total_value) AS order_value
FROM transactions
GROUP BY customer_id, DATE(transaction_date);
"""
df_orders = run_sql_pure(sql)
aov = df_orders['order_value'].mean()
print(f"AOV (m√©dia de valor por pedido - aproxima√ß√£o): R$ {aov:.2f}")
plt.hist(df_orders['order_value'], bins=30)
plt.title("Distribui√ß√£o do valor por pedido (aprox.)")
plt.xlabel("Order value")
plt.ylabel("Contagem")
plt.show()
```
<br>

##### Matriz cliente x produto - densidade (SPARSITY)
```{python}
sql = """
SELECT (SELECT COUNT(DISTINCT customer_id) FROM transactions) AS n_customers_active,
       (SELECT COUNT(DISTINCT product_id) FROM transactions) AS n_products_sold,
       (SELECT COUNT(*) FROM transactions) AS n_transactions;
"""
df_dim = run_sql_pure(sql)
n_customers = int(df_dim['n_customers_active'].iloc[0])
n_products = int(df_dim['n_products_sold'].iloc[0])
n_trans = int(df_dim['n_transactions'].iloc[0])
possible = n_customers * n_products
sparsity = 1 - (n_trans / possible)
print(f"Clientes: {n_customers}, Produtos: {n_products}, Transa√ß√µes: {n_trans}")
print(f"Sparsity (aprox): {sparsity:.6f}")
```

---

## Key Insights from SQL EDA

A an√°lise revela padr√µes importantes:

* alta concentra√ß√£o de vendas em poucas categorias,
* clientes com frequ√™ncia relativamente elevada,
* sparsity alta (>90%) na matriz de intera√ß√µes.

Esses achados justificam a escolha por:

* modelos **item-based** em vez de user-based,
* abordagem h√≠brida para mitigar limita√ß√µes individuais.

---

# Cria√ß√£o dos Algoritimos

Com base na EDA, foi definida uma arquitetura h√≠brida onde cada componente resolve um problema espec√≠fico:

| Componente        | Papel                                   |
| ----------------- | --------------------------------------- |
| Item-Item CF      | Personaliza√ß√£o baseada em comportamento |
| Content-Based     | Similaridade sem√¢ntica e cold start     |
| Association Rules | Cross-sell contextual                   |

O objetivo n√£o √© substituir modelos, mas **combinar sinais complementares**.


## Item-Item Collaborative Filtering

Nesta etapa, constru√≠mos um sistema de recomenda√ß√£o baseado exclusivamente em padr√µes hist√≥ricos de compra. O Item-Item CF recomenda itens semelhantes aos que o usu√°rio j√° comprou, medindo similaridade entre itens a partir do comportamento dos usu√°rios (quem comprou A tamb√©m comprou B).

```{python}
#| include: false
# Carregando Datasets
customers = pd.read_csv("customers.csv")
products = pd.read_csv("products.csv")
transactions = pd.read_csv("transactions.csv")
views = pd.read_csv("product_views.csv")
```

##### Criar dataset de intera√ß√µes (cliente x produto):
```{python}
interactions = (
    transactions[['customer_id', 'product_id']]
    .drop_duplicates()
)
interactions['interaction'] = 1
interactions.head()
```
<br>

##### Construir a matriz Item x Cliente:
```{python}
item_user_matrix = interactions.pivot_table(
    index='product_id',
    columns='customer_id',
    values='interaction',
    fill_value=0
)

item_user_matrix.shape
```
<br>

##### Checar Sparsity:
```{python}
density = item_user_matrix.values.sum() / item_user_matrix.size
sparsity = 1 - density

print(f"Sparsity: {sparsity:.4f}")
```
<br>

##### Calcular Similaridade Item-Item (Cosine):
```{python}
item_similarity = cosine_similarity(item_user_matrix)

item_sim_df = pd.DataFrame(
    item_similarity,
    index=item_user_matrix.index,
    columns=item_user_matrix.index
)

item_sim_df.iloc[:5, :5]
```
<br>

##### Co-ocorr√™ncia
```{python}
cooccurrence = item_user_matrix @ item_user_matrix.T
```
<br>

##### Shrinkage
```{python}
LAMBDA = 10
shrunk_similarity = item_sim_df * (cooccurrence / (cooccurrence + LAMBDA))
```
<br>

##### Fun√ß√£o de Recomenda√ß√£o (Item-Item CF)
```{python}
# Fun√ß√£o de Recomenda√ß√£o (Item-Item CF)
def recommend_item_item(user_id, k=10):
    # Produtos comprados pelo usu√°rio
    bought_items = interactions.loc[
        interactions['customer_id'] == user_id, 'product_id'
    ].unique()

    if len(bought_items) == 0:
        return pd.Series(dtype=float)

    # Score m√©dio de similaridade
    scores = shrunk_similarity.loc[bought_items].mean(axis=0)

    # Remover itens j√° comprados
    scores = scores.drop(bought_items, errors='ignore')

    return scores.sort_values(ascending=False).head(k)
```
<br>

##### Teste do modelo
```{python}
test_user = interactions['customer_id'].iloc[0]
recommend_item_item(test_user, k=5)
```
<br>

##### Enriquecer com Informa√ß√µes do Produto
```{python}
def recommend_with_metadata(user_id, k=10):
    recs = recommend_item_item(user_id, k)
    
    return (
        recs
        .reset_index()
        .merge(products, on='product_id', how='left')
        .rename(columns={0: 'score'})
    )

recommend_with_metadata(test_user, k=5)
```
<br>

O modelo Item-Item Collaborative Filtering identifica produtos semelhantes a partir de padr√µes de compra reais, permitindo recomenda√ß√µes personalizadas e escal√°veis. A aplica√ß√£o de shrinkage reduz ru√≠do causado por co-ocorr√™ncias raras, tornando o sistema mais confi√°vel em cen√°rios de alta esparsidade, t√≠picos de e-commerce.

O score gerado pelo Item-Item CF representa uma medida relativa de afinidade entre produtos, baseada exclusivamente em padr√µes hist√≥ricos de compra. Ele √© utilizado para ranquear itens recomendados e n√£o deve ser interpretado como probabilidade absoluta.

---

# Content-Based Filtering (TF-IDF)

Apesar da for√ßa do sinal comportamental, o CF apresenta limita√ß√µes:

| Limita√ß√£o                | Impacto                          |
| ------------------------ | -------------------------------- |
| Cold start de produto    | Produto novo nunca √© recomendado |
| Depend√™ncia do hist√≥rico | Pouca explica√ß√£o sem√¢ntica       |
| Sparsity alta            | Alguns itens quase n√£o aparecem  |

Para mitigar isso, utilizamos Content-Based Filtering, representando produtos por seus atributos textuais.

##### Prepara√ß√£o do texto dos produtos:
```{python}
products_cb = products.copy()

products_cb['text'] = (
    products_cb['name'].fillna('') + ' ' +
    products_cb['category'].fillna('') + ' ' +
    products_cb['brand'].fillna('')
)

products_cb[['product_id', 'text']].head()
```
<br>

##### Vetoriza√ß√£o com TF-IDF:
```{python}
tfidf = TfidfVectorizer(
    ngram_range=(1,2),
    min_df=2
)

tfidf_matrix = tfidf.fit_transform(products_cb['text'])

tfidf_matrix.shape
```
<br>

##### Similaridade entre produtos (Cosine):
```{python}
content_similarity = cosine_similarity(tfidf_matrix)
```
<br>

##### Criar matriz Produto x Produto:
```{python}
content_sim_df = pd.DataFrame(
    content_similarity,
    index=products_cb['product_id'],
    columns=products_cb['product_id']
)

content_sim_df.iloc[:5, :5]
```
<br>

##### Fun√ß√£o de recomenda√ß√£o Content-Based:
```{python}
def recommend_content_based(user_id, k=10):
    bought_items = interactions.loc[
        interactions['customer_id'] == user_id, 'product_id'
    ].unique()

    if len(bought_items) == 0:
        return pd.Series(dtype=float)

    scores = content_sim_df.loc[bought_items].mean(axis=0)
    scores = scores.drop(bought_items, errors='ignore')

    return scores.sort_values(ascending=False).head(k)
```
<br>

##### Teste do Content-Based:
```{python}
test_user = interactions['customer_id'].iloc[0]
recommend_content_based(test_user, k=5)
```
<br>

##### Enriquecer com informa√ß√µes do produto:
```{python}
def recommend_cb_with_metadata(user_id, k=10):
    recs = recommend_content_based(user_id, k)
    return (
        recs
        .reset_index()
        .merge(products, on='product_id', how='left')
        .rename(columns={0: 'score'})
    )

recommend_cb_with_metadata(test_user, k=5)
```
<br>

O modelo baseado em conte√∫do recomenda produtos semanticamente similares, ampliando a cobertura do sistema e permitindo recomenda√ß√µes mesmo para itens com pouco hist√≥rico.

---

# Offline Evaluation ‚Äî CF vs CB

Antes de combinar os modelos em uma abordagem h√≠brida, avaliamos separadamente o Item-Item Collaborative Filtering (CF) e o Content-Based Filtering (CB). Essa etapa √© fundamental para entender a contribui√ß√£o individual de cada sinal, identificar suas for√ßas e limita√ß√µes e evitar que um modelo dominante mas ruidoso prejudique o desempenho final.

A avalia√ß√£o isolada permite comparar desempenho em m√©tricas de ranking, ajustar pesos do modelo h√≠brido de forma fundamentada e justificar tecnicamente a escolha da arquitetura. Em cen√°riosde e-commerce, essa pr√°tica √© essencial para garantir que o modelo h√≠brido gere ganho incremental em rela√ß√£o aos baselines individuais, em vez de apenas misturar sinais sem benef√≠cio mensur√°vel.

##### Cria um split temporal do tipo Leave-Last-Out para avalia√ß√£o:
```{python}
interactions_eval = transactions[['customer_id', 'product_id', 'transaction_date']].copy()

# Garantir tipo datetime
interactions_eval['transaction_date'] = pd.to_datetime(
    interactions_eval['transaction_date']
)

# Ordenar temporalmente
interactions_eval = interactions_eval.sort_values(
    ['customer_id', 'transaction_date', 'product_id']
)

# Inicializar estruturas
train_interactions = []
test_interactions = {}

# Holdout temporal (Last Item)
for user_id, group in interactions_eval.groupby('customer_id'):
    items = group['product_id'].tolist()

    if len(items) < 2:
        continue

    test_item = items[-1]  # √∫ltimo item (mais recente)
    test_interactions[user_id] = test_item

    for item in items[:-1]:
        train_interactions.append((user_id, item))

train_df = pd.DataFrame(
    train_interactions,
    columns=['customer_id', 'product_id']
)
train_df['interaction'] = 1
```
<br>

##### Recriar a matriz Item x User para trein:
```{python}
item_user_train = train_df.pivot_table(
    index='product_id',
    columns='customer_id',
    values='interaction',
    fill_value=0
)
```
<br>

#### Recalcular CF treino:
```{python}
item_sim_train = cosine_similarity(item_user_train)
item_sim_train_df = pd.DataFrame(
    item_sim_train,
    index=item_user_train.index,
    columns=item_user_train.index
)
```
<br>

##### Fun√ß√£o de recomenda√ß√£o para avalia√ß√£o (CF)
```{python}
def recommend_cf_eval(user_id, k=10):
    bought_items = train_df.loc[
        train_df['customer_id'] == user_id, 'product_id'
    ].unique()

    if len(bought_items) == 0:
        return []

    scores = item_sim_train_df.loc[bought_items].mean(axis=0)
    scores = scores.drop(bought_items, errors='ignore')

    return scores.sort_values(ascending=False).head(k).index.tolist()
```
<br>

##### Fun√ß√£o de recomenda√ß√£o para avalia√ß√£o:
```{python}
def recommend_cb_eval(user_id, k=10):
    bought_items = train_df.loc[
        train_df['customer_id'] == user_id, 'product_id'
    ].unique()

    if len(bought_items) == 0:
        return []

    scores = content_sim_df.loc[bought_items].mean(axis=0)
    scores = scores.drop(bought_items, errors='ignore')

    return scores.sort_values(ascending=False).head(k).index.tolist()
```
<br>

##### Avaliar Hit Rate @ K:
```{python}
def hit_rate(model_func, k=10):
    hits = 0
    total = 0

    for user_id in sorted(test_interactions.keys()):
        true_item = test_interactions[user_id]

        recs = model_func(user_id, k)

        if true_item in recs:
            hits += 1

        total += 1

    return hits / total
```
<br>

##### Resultados ‚Äî CF vs CB:
```{python}
for k in [5, 10]:
    hr_cf = hit_rate(recommend_cf_eval, k)
    hr_cb = hit_rate(recommend_cb_eval, k)

    print(f"Hit Rate @ {k}")
    print(f"  Item-Item CF: {hr_cf:.4f}")
    print(f"  Content-Based: {hr_cb:.4f}")
    print("-" * 30)
```
<br>

Antes da constru√ß√£o do modelo h√≠brido, avaliamos separadamente o desempenho do Item-Item Collaborative Filtering e do Content-Based Filtering utilizando uma estrat√©gia Leave-One-Out e a m√©trica Hit Rate@K.

Os resultados indicam que o modelo colaborativo apresenta maior capacidade preditiva, refletindo a for√ßa do sinal comportamental em dados impl√≠citos. O Content-Based, embora com desempenho inferior isoladamente, mostrou-se consistente e adequado para complementar o sistema, especialmente em cen√°rios de cold start e aumento de cobertura.

**Conclus√£o**:
O CF apresenta melhor desempenho isolado, enquanto o CB agrega valor incremental ‚Äî justificando a combina√ß√£o.

# Hybrid Model ‚Äî Weight Tuning

Nesta etapa, testamos diferentes combina√ß√µes de pesos entre CF e CB para maximizar performance.

#### Vamos testar:

| w_cf | w_cb |
| ---- | ---- |
| 0.0  | 1.0  |
| 0.2  | 0.8  |
| 0.4  | 0.6  |
| 0.6  | 0.4  |
| 0.8  | 0.2  |
| 1.0  | 0.0  |


Isso inclui:
* CB puro
* CF puro
* H√≠bridos intermedi√°rios

##### Fun√ß√£o utilit√°ria de normaliza√ß√£o:
```{python}
def min_max_normalize(scores: pd.Series):
    if scores.empty:
        return scores
    return (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)
```
<br>

##### Fun√ß√£o h√≠brida de recomenda√ß√£o:
```{python}
def recommend_hybrid_eval(
    user_id,
    k=10,
    w_cf=0.6,
    w_cb=0.4
):

    bought_items = train_df.loc[
        train_df['customer_id'] == user_id, 'product_id'
    ].unique()

    if len(bought_items) == 0:
        return pd.Series(dtype=float)

    # CF
    scores_cf = item_sim_train_df.loc[bought_items].mean(axis=0)
    scores_cf = scores_cf.drop(bought_items, errors='ignore')
    scores_cf = min_max_normalize(scores_cf)

    # CB
    scores_cb = content_sim_df.loc[bought_items].mean(axis=0)
    scores_cb = scores_cb.drop(bought_items, errors='ignore')
    scores_cb = min_max_normalize(scores_cb)

    # Combina√ß√£o
    hybrid_scores = (
        w_cf * scores_cf +
        w_cb * scores_cb
    ).dropna()

    return hybrid_scores.sort_values(ascending=False).head(k)
```
<br>

##### Fun√ß√£o de avalia√ß√£o do h√≠brido:
```{python}
def evaluate_hybrid(weights, k=10):
    results = []

    # Ordem fixa de usu√°rios
    users = sorted(test_interactions.keys())

    for w_cf, w_cb in weights:
        hits = []

        for user in users:
            true_item = test_interactions[user]

            recs = recommend_hybrid_eval(
                user_id=user,
                k=k,
                w_cf=w_cf,
                w_cb=w_cb
            )

            hits.append(int(true_item in recs.index))

        results.append({
            'w_cf': w_cf,
            'w_cb': w_cb,
            f'hit_rate@{k}': np.mean(hits)
        })

    return pd.DataFrame(results)
```
<br>

##### Rodar o tuning:
```{python}
weight_grid = [
    (0.0, 1.0),
    (0.2, 0.8),
    (0.4, 0.6),
    (0.6, 0.4),
    (0.8, 0.2),
    (1.0, 0.0),
]

df_hybrid_eval = evaluate_hybrid(weight_grid, k=10)
df_hybrid_eval.sort_values('hit_rate@10', ascending=False)
```
<br>


A combina√ß√£o dos modelos colaborativo e baseado em conte√∫do resultou em melhoria consistente de desempenho. O melhor resultado foi obtido com pesos w_cf=0.6 e w_cb=0.4, evidenciando que o sinal comportamental deve ser predominante, mas complementado por similaridade sem√¢ntica.

Esses resultados confirmam que o modelo h√≠brido captura m√∫ltiplas dimens√µes do comportamento do usu√°rio, reduz limita√ß√µes individuais dos modelos base e oferece um trade-off mais robusto entre precis√£o e cobertura.


# Association Rules (Apriori)

Al√©m das abordagens colaborativa e baseada em conte√∫do, o sistema incorpora Association Rules para capturar padr√µes expl√≠citos de coocorr√™ncia em cestas de compra. Essa t√©cnica permite identificar produtos frequentemente adquiridos juntos, sendo especialmente eficaz para estrat√©gias de cross-sell e aumento do ticket m√©dio.

Diferentemente dos modelos centrados no usu√°rio, as regras de associa√ß√£o operam diretamente sobre transa√ß√µes, tornando-se robustas a cen√°rios de cold start de usu√°rios e altamente interpret√°veis para stakeholders de neg√≥cio. Integradas ao modelo h√≠brido, essas regras complementam a personaliza√ß√£o com recomenda√ß√µes de produtos complementares.
 
Analogia simples
CF = sugerir outra camisa parecida
CB = sugerir camisa da mesma marca
Association Rules = sugerir o cinto que normalmente vai junto com a camisa

Ou seja, responder a pergunta: O que costuma ser comprado junto?

##### Criar as cestas:
```{python}
# Garantir que a coluna 'transaction_date' seja do tipo datetime
transactions['transaction_date'] = pd.to_datetime(transactions['transaction_date'])

# Criar uma coluna de "order_id"
transactions['order_id'] = (
    transactions['customer_id'].astype(str) + '_' +
    transactions['transaction_date'].dt.date.astype(str)
)

# Agrupar produtos por pedido
basket = (
    transactions
    .groupby('order_id')['product_id']
    .apply(list)
)

basket.head()
```
<br>

##### One-Hot Encoding das cestas:
```{python}
te = TransactionEncoder()
te_array = te.fit(basket).transform(basket)

basket_df = pd.DataFrame(
    te_array,
    columns=te.columns_
)

basket_df.head()
```
<br>

##### Loop por categoria:
```{python}

```
<br>


```{python}
rules_all = []

for category in products['category'].unique():

    products_cat = products.loc[
        products['category'] == category, 'product_id'
    ].tolist()

    basket_cat = basket.apply(
        lambda items: [i for i in items if i in products_cat]
    )

    basket_cat = basket_cat[basket_cat.apply(len) >= 2]

    if len(basket_cat) < 50:
        continue

    te = TransactionEncoder()
    te_array = te.fit(basket_cat).transform(basket_cat)

    basket_df_cat = pd.DataFrame(
        te_array,
        columns=te.columns_
    )

    frequent_itemsets = apriori(
        basket_df_cat,
        min_support=0.01,
        use_colnames=True,
        max_len=2
    )

    if frequent_itemsets.empty:
        continue

    rules = association_rules(
        frequent_itemsets,
        metric='lift',
        min_threshold=1.0
    )

    if rules.empty:
        continue

    rules['category'] = category
    rules_all.append(rules)
```
<br>

##### Consolidar todas as regras:
```{python}
rules_df = pd.concat(rules_all, ignore_index=True)
rules_df.sort_values('lift', ascending=False).head(10)
```
<br>

##### Filtrar regras:
```{python}
rules_filtered = rules_df[
    (rules_df['antecedents'].apply(len) == 1) &
    (rules_df['consequents'].apply(len) == 1) &
    (rules_df['confidence'] >= 0.2) &
    (rules_df['lift'] >= 1.2)
].sort_values('lift', ascending=False)

rules_filtered.head(10)
```
<br>

##### Fun√ß√£o de recomenda√ß√£o ‚Äî Association Rules por categoria:
```{python}
def recommend_association_by_category(user_id, k=5):

    bought_items = interactions.loc[
        interactions['customer_id'] == user_id, 'product_id'
    ].unique()

    if len(bought_items) == 0:
        return pd.Series(dtype=float)

    recs = []

    for item in bought_items:

        item_category = products.loc[
            products['product_id'] == item, 'category'
        ].values[0]

        matched_rules = rules_filtered[
            (rules_filtered['category'] == item_category) &
            (rules_filtered['antecedents'].apply(lambda x: item in x))
        ]

        for _, row in matched_rules.iterrows():
            consequent = list(row['consequents'])[0]
            recs.append((consequent, row['lift']))

    if not recs:
        return pd.Series(dtype=float)

    recs_df = pd.DataFrame(recs, columns=['product_id', 'score'])

    return (
        recs_df
        .groupby('product_id')['score']
        .max()
        .sort_values(ascending=False)
        .head(k)
    )
```
<br>

##### Teste:
```{python}
user_items = interactions.loc[
    interactions['customer_id'] == test_user, 'product_id'
].unique()

user_items
```
<br>

As regras de associa√ß√£o foram aplicadas como um sinal complementar de cross-sell, sendo utilizadas apenas quando o hist√≥rico do usu√°rio intersecta os antecedentes das regras extra√≠das. Em cen√°rios onde n√£o h√° correspond√™ncia, o sistema n√£o for√ßa recomenda√ß√µes artificiais, preservando a qualidade do ranking

# Final Hybrid Recommender

O modelo final combina:

| Componente        | Papel no sistema                        |
| ----------------- | --------------------------------------- |
| Item-Item CF      | Personaliza√ß√£o baseada em comportamento |
| Content-Based     | Similaridade sem√¢ntica / cold start     |
| Association Rules | Cross-sell contextual (checkout)        |

O h√≠brido n√£o substitui, ele combina sinais.

Obs: Cada modelo gera scores em escalas diferentes. Precisamos normalizar antes de somar.

##### Fun√ß√£o utilit√°ria de normaliza√ß√£o:
```{python}
def min_max_normalize(scores: pd.Series):
    if scores.empty:
        return scores
    return (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)
```
<br>

##### Fun√ß√£o de recomenda√ß√£o h√≠brida:
```{python}
def recommend_hybrid(
    user_id,
    k=10,
    w_cf=0.5,
    w_cb=0.3,
    w_ar=0.2
):

    scores_final = pd.Series(dtype=float)

    # ---------- CF ----------
    scores_cf = recommend_item_item(user_id, k=50)
    scores_cf = min_max_normalize(scores_cf)

    if not scores_cf.empty:
        scores_final = scores_final.add(w_cf * scores_cf, fill_value=0)

    # ---------- Content-Based ----------
    scores_cb = recommend_content_based(user_id, k=50)
    scores_cb = min_max_normalize(scores_cb)

    if not scores_cb.empty:
        scores_final = scores_final.add(w_cb * scores_cb, fill_value=0)

    # ---------- Association Rules ----------
    scores_ar = recommend_association_by_category(user_id, k=50)
    scores_ar = min_max_normalize(scores_ar)

    if not scores_ar.empty:
        scores_final = scores_final.add(w_ar * scores_ar, fill_value=0)

    # ---------- Remover itens j√° comprados ----------
    bought_items = interactions.loc[
        interactions['customer_id'] == user_id, 'product_id'
    ].unique()

    scores_final = scores_final.drop(bought_items, errors='ignore')

    return scores_final.sort_values(ascending=False).head(k)
```
<br>

##### Teste do h√≠brido:
```{python}
test_user = interactions['customer_id'].iloc[0]
recommend_hybrid(test_user, k=10)
```
<br>

##### Enriquecer com metadata:

```{python}
def recommend_hybrid_with_metadata(user_id, k=10):

    recs = recommend_hybrid(user_id, k)

    return (
        recs
        .reset_index()
        .rename(columns={'index': 'product_id', 0: 'score'})
        .merge(products, on='product_id', how='left')
    )

recommend_hybrid_with_metadata(test_user, k=10)
```
<br>

O sistema final combina tr√™s sinais complementares: Item-Item Collaborative Filtering, Content-Based Filtering e Association Rules. Cada componente atua em um aspecto distinto do problema, garantindo personaliza√ß√£o, cobertura e capacidade de cross-sell. As pontua√ß√µes s√£o normalizadas e combinadas por meio de uma soma ponderada, permitindo ajuste fino do impacto de cada sinal. Essa abordagem reflete pr√°ticas reais de mercado, onde sistemas h√≠bridos s√£o preferidos por sua robustez e flexibilidade.

O modelo analisa o hist√≥rico do cliente, a similaridade entre produtos e padr√µes recorrentes de compra conjunta para gerar recomenda√ß√µes personalizadas e contextualizadas, equilibrando relev√¢ncia individual e oportunidades de aumento de ticket m√©dio. Ou seja, em cen√°rios onde n√£o h√° correspond√™ncia, o sistema n√£o for√ßa recomenda√ß√µes artificiais, preservando a qualidade do ranking.

# Conclus√£o

Foi desenvolvido um sistema h√≠brido de recomenda√ß√£o que combina sinais comportamentais (Item-Item Collaborative Filtering), sem√¢nticos (Content-Based Filtering) e contextuais (Association Rules). A arquitetura garante cobertura, personaliza√ß√£o e capacidade de cross-sell, respeitando limita√ß√µes estat√≠sticas dos dados e evitando recomenda√ß√µes artificiais. O modelo reflete pr√°ticas reais de produ√ß√£o e foi avaliado de forma incremental, demonstrando ganhos qualitativos em relev√¢ncia e interpretabilidade.




```{python}

```
<br>








# Offline Artifacts & Deployment Strategy

Os modelos s√£o treinados offline e serializados como artefatos est√°ticos.

Em produ√ß√£o:

> *A API apenas carrega artefatos e executa ranking.*

‚¨áÔ∏è **INSIRA O C√ìDIGO AQUI (serializa√ß√£o)**

---

# Cloud Deployment (AWS)

A solu√ß√£o √© implantada em arquitetura serverless:

* S3 para armazenamento de artefatos,
* Lambda para infer√™ncia,
* API Gateway para exposi√ß√£o REST.

üé• **Video ‚Äî AWS Deploy & API Demo**

```markdown
{{< video aws_demo.mp4 >}}
```

---

# Conclusion

Este projeto demonstra a constru√ß√£o de um sistema de recomenda√ß√£o h√≠brido **robusto, interpret√°vel e orientado √† produ√ß√£o**, combinando rigor t√©cnico com impacto direto no neg√≥cio.

A abordagem h√≠brida permite equilibrar personaliza√ß√£o, cobertura e oportunidades de cross-sell, refletindo pr√°ticas reais adotadas em plataformas de e-commerce em escala.

---

## GitHub Repository

[Link para o reposit√≥rio]

---


